name: Enhanced Next Random Number Identifier CI/CD

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  schedule:
    # Run weekly on Monday at 00:00 UTC
    - cron: '0 0 * * 1'

jobs:
  validate-documentation:
    runs-on: ubuntu-latest
    name: Validate Documentation
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install sphinx sphinx_rtd_theme pytest pytest-checkdocs
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Check for architectural changes
      id: architecture-changes
      run: |
        # Files that might indicate architectural changes
        ARCH_FILES="src/models/base_model.py src/models/ensemble.py src/features/feature_engineering.py src/**/base*.py"
        
        # Check if architectural files were modified
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }} | grep -E "$ARCH_FILES" > arch_changes.txt || true
        else
          # For push events, compare with the parent commit
          git diff --name-only HEAD^ HEAD | grep -E "$ARCH_FILES" > arch_changes.txt || true
        fi
        
        if [ -s arch_changes.txt ]; then
          echo "architecture_changes=true" >> $GITHUB_OUTPUT
          echo "Changed architectural files:"
          cat arch_changes.txt
        else
          echo "architecture_changes=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Check documentation updates
      id: doc-updates
      if: steps.architecture-changes.outputs.architecture_changes == 'true'
      run: |
        # Check if docs were updated
        if [ "${{ github.event_name }}" == "pull_request" ]; then
          git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }} | grep -E "docs/" > doc_changes.txt || true
        else
          # For push events, compare with the parent commit
          git diff --name-only HEAD^ HEAD | grep -E "docs/" > doc_changes.txt || true
        fi
        
        if [ -s doc_changes.txt ]; then
          echo "documentation_updated=true" >> $GITHUB_OUTPUT
          echo "Documentation files updated:"
          cat doc_changes.txt
        else
          echo "documentation_updated=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Validate diagrams
      id: diagram-validation
      run: |
        # Check if diagrams directory exists first
        if [ ! -d "docs/diagrams" ]; then
          echo "::warning::docs/diagrams directory not found. Skipping diagram validation."
          echo "MISSING_DIAGRAMS_DIR=true" >> $GITHUB_OUTPUT
          exit 0
        fi
        
        cd docs/diagrams
        
        # Handle case where no .mermaid files exist
        if [ ! "$(ls -A *.mermaid 2>/dev/null)" ]; then
          echo "No .mermaid files found in diagrams directory"
        else
          # Check for .mermaid files without corresponding .png files
          for file in *.mermaid; do
            png_file="${file%.mermaid}.png"
            if [ ! -f "$png_file" ]; then
              echo "::warning::Missing PNG for $file"
              echo "MISSING_PNGS=true" >> $GITHUB_OUTPUT
            fi
          done
        fi
        
        # Handle case where no .png files exist
        if [ ! "$(ls -A *.png 2>/dev/null)" ]; then
          echo "No .png files found in diagrams directory"
        else
          # Check for .png files without corresponding .mermaid files (orphaned PNGs)
          for file in *.png; do
            mermaid_file="${file%.png}.mermaid"
            if [ ! -f "$mermaid_file" ]; then
              echo "::warning::Orphaned PNG without source: $file"
              echo "ORPHANED_PNGS=true" >> $GITHUB_OUTPUT
            fi
          done
        fi
        
        # Check timestamps if both file types exist
        if [ "$(ls -A *.mermaid 2>/dev/null)" ] && [ "$(ls -A *.png 2>/dev/null)" ]; then
          for file in *.mermaid; do
            png_file="${file%.mermaid}.png"
            if [ -f "$png_file" ]; then
              # Use ls -la to get file timestamps in a more portable way
              mermaid_time=$(stat -c %Y "$file" 2>/dev/null || stat -f %m "$file" 2>/dev/null)
              png_time=$(stat -c %Y "$png_file" 2>/dev/null || stat -f %m "$png_file" 2>/dev/null)
              
              if [ -n "$mermaid_time" ] && [ -n "$png_time" ] && [ "$mermaid_time" -gt "$png_time" ]; then
                echo "::warning::Diagram $file is newer than its PNG rendering"
                echo "OUT_OF_SYNC_DIAGRAMS=true" >> $GITHUB_OUTPUT
              fi
            fi
          done
        fi
      continue-on-error: true
    
    - name: Validate code-doc interface consistency
      run: |
        # Check if docs directory exists
        if [ ! -d "docs" ]; then
          echo "::warning::docs directory not found. Skipping code-doc interface validation."
          exit 0
        fi
        
        # Use grep to extract interface method signatures from docs
        grep -r --include="*.md" -E "def [a-zA-Z_]+" docs/ > doc_methods.txt || true
        
        # Check if methods from docs exist in actual code
        if [ -s doc_methods.txt ]; then
          echo "Checking documented methods against actual code..."
          while IFS= read -r line; do
            # Extract method name
            method_name=$(echo "$line" | grep -oE "def [a-zA-Z_]+" | cut -d ' ' -f 2)
            if [ -n "$method_name" ]; then
              # Check if method exists in code
              if ! grep -r --include="*.py" -E "def $method_name" src/ > /dev/null; then
                echo "::warning::Method documented but not found in code: $method_name"
              fi
            fi
          done < doc_methods.txt
        fi
      continue-on-error: true
    
    - name: Documentation validation summary
      run: |
        echo "Documentation Validation Summary:"
        
        if [ "${{ steps.architecture-changes.outputs.architecture_changes }}" == "true" ]; then
          echo "- Architectural changes detected"
          
          if [ "${{ steps.doc-updates.outputs.documentation_updated }}" != "true" ]; then
            echo "::warning::Architectural changes detected but documentation was not updated"
          else
            echo "- Documentation was updated with architectural changes ✓"
          fi
        else
          echo "- No architectural changes detected"
        fi
        
        if [ "${{ steps.diagram-validation.outputs.MISSING_PNGS }}" == "true" ] || \
           [ "${{ steps.diagram-validation.outputs.ORPHANED_PNGS }}" == "true" ] || \
           [ "${{ steps.diagram-validation.outputs.OUT_OF_SYNC_DIAGRAMS }}" == "true" ]; then
          echo "::warning::Diagram inconsistencies detected. Please run maintenance/maintain_diagrams.py"
        else
          echo "- All diagrams are consistent ✓"
        fi
        
        # Documentation validation check passed if:
        # 1. No architectural changes OR documentation was updated with architectural changes
        # 2. All diagrams are consistent
        if { [ "${{ steps.architecture-changes.outputs.architecture_changes }}" != "true" ] || \
             [ "${{ steps.doc-updates.outputs.documentation_updated }}" == "true"; } && \
           [ "${{ steps.diagram-validation.outputs.MISSING_PNGS }}" != "true" ] && \
           [ "${{ steps.diagram-validation.outputs.ORPHANED_PNGS }}" != "true" ] && \
           [ "${{ steps.diagram-validation.outputs.OUT_OF_SYNC_DIAGRAMS }}" != "true" ]; then
          echo "- Documentation validation passed ✓"
          exit 0
        else
          echo "::warning::Documentation validation found issues that should be addressed"
          # Don't fail the build, just warn
          exit 0
        fi
      continue-on-error: true

  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest pytest-cov flake8 black isort mypy
    
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Check formatting with black
      run: |
        black --check src tests
      continue-on-error: true
    
    - name: Check imports with isort
      run: |
        isort --check-only --profile black src tests
      continue-on-error: true
    
    - name: Type check with mypy
      run: |
        mypy src
      continue-on-error: true
    
    - name: Test with pytest
      run: |
        pytest --cov=src tests/ --cov-report=xml
    
    - name: Upload coverage report
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  benchmark:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest-benchmark
    
    - name: Run benchmarks
      run: |
        python -c "
import os
import json
import pandas as pd
from src.models.random_forest import RandomForestModel
from src.models.xgboost_model import XGBoostModel
from src.models.markov_chain import MarkovChain
from src.models.ensemble import EnhancedEnsemble
from src.features.feature_engineering import FeatureEngineer
from src.utils.data_loader import DataLoader

# Ensure data directory exists
os.makedirs('benchmark_results', exist_ok=True)

# Load and prepare data
data_loader = DataLoader('data')
df = data_loader.load_csv('historical_random_numbers.csv')
df = data_loader.preprocess_data(df)

# Create features
feature_engineer = FeatureEngineer()
df_features = feature_engineer.transform(df)

# Prepare data for models
X = df_features.drop(['Date', 'Number'], axis=1).fillna(0)
y = df_features['Number'].fillna(df_features['Number'].median())

# Initialize models
models = {
    'RandomForest': RandomForestModel(),
    'XGBoost': XGBoostModel(),
    'MarkovChain': MarkovChain(),
    'Ensemble': EnhancedEnsemble()
}

# Train and benchmark each model
benchmark_results = {}
for name, model in models.items():
    # Measure training time
    import time
    start_time = time.time()
    model.fit(X, y)
    training_time = time.time() - start_time
    
    # Measure prediction time
    start_time = time.time()
    predictions = model.predict(X.head(100))
    prediction_time = time.time() - start_time
    
    # Store results
    benchmark_results[name] = {
        'training_time': training_time,
        'prediction_time': prediction_time,
        'training_samples_per_second': len(X) / training_time,
        'prediction_samples_per_second': 100 / prediction_time
    }

# Save benchmark results
with open('benchmark_results/model_performance.json', 'w') as f:
    json.dump(benchmark_results, f, indent=2)

print('Benchmark results:', benchmark_results)
        "
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results/

  build-docs:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install sphinx sphinx_rtd_theme
    
    - name: Build documentation
      run: |
        mkdir -p docs
        sphinx-quickstart -q -p "Next Random Number Identifier" -a "AIQube Centaur Systems Team" -v "2.0" --ext-autodoc --ext-viewcode docs
        sphinx-apidoc -o docs/source src
        cd docs && make html
    
    - name: Deploy documentation
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs/build/html

  model-drift-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Download previous model metrics
      uses: actions/download-artifact@v3
      with:
        name: model-metrics
        path: previous-metrics
      continue-on-error: true
    
    - name: Run model drift check
      run: |
        python -c "
import os
import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from src.models.random_forest import RandomForestModel
from src.models.xgboost_model import XGBoostModel
from src.models.markov_chain import MarkovChain
from src.models.ensemble import EnhancedEnsemble
from src.features.feature_engineering import FeatureEngineer
from src.utils.data_loader import DataLoader
from src.utils.monitoring import ModelMonitor

# Ensure directories exist
os.makedirs('model-metrics', exist_ok=True)

# Load and prepare data
data_loader = DataLoader('data')
df = data_loader.load_csv('historical_random_numbers.csv')
df = data_loader.preprocess_data(df)

# Create features
feature_engineer = FeatureEngineer()
df_features = feature_engineer.transform(df)

# Prepare data for models
X = df_features.drop(['Date', 'Number'], axis=1).fillna(0)
y = df_features['Number'].fillna(df_features['Number'].median())

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'RandomForest': RandomForestModel(),
    'XGBoost': XGBoostModel(),
    'MarkovChain': MarkovChain(),
    'Ensemble': EnhancedEnsemble()
}

# Train and evaluate each model
current_metrics = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    if hasattr(model, 'evaluate'):
        metrics = model.evaluate(X_test, y_test)
    else:
        # Fallback for models without evaluate method
        preds = model.predict(X_test)
        from sklearn.metrics import mean_squared_error
        metrics = {'mse': mean_squared_error(y_test, preds)}
    current_metrics[name] = metrics

# Save current metrics
with open('model-metrics/current_metrics.json', 'w') as f:
    json.dump(current_metrics, f, indent=2)

# Check for model drift if previous metrics exist
drift_detected = False
if os.path.exists('previous-metrics/current_metrics.json'):
    try:
        with open('previous-metrics/current_metrics.json', 'r') as f:
            previous_metrics = json.load(f)
        
        monitor = ModelMonitor()
        
        for model_name in current_metrics.keys():
            if model_name in previous_metrics:
                monitor.set_baseline(previous_metrics[model_name])
                monitor.track_performance(current_metrics[model_name])
                drift_summary = monitor.get_drift_summary()
                
                if drift_summary.get('drift_detected', False):
                    drift_detected = True
                    print(f'Drift detected for model {model_name}')
                    print(drift_summary)
        
        if drift_detected:
            print('WARNING: Model drift detected! Performance has changed significantly.')
        else:
            print('No significant model drift detected.')
    except Exception as e:
        print(f'Error checking for model drift: {str(e)}')
else:
    print('No previous metrics found. This will be the baseline for future comparisons.')

# Upload current metrics as artifact
print('Current model metrics:', current_metrics)
        "
    
    - name: Upload model metrics
      uses: actions/upload-artifact@v3
      with:
        name: model-metrics
        path: model-metrics/

  package:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install setuptools wheel twine
    
    - name: Build package
      run: |
        python setup.py sdist bdist_wheel
    
    - name: Upload distribution packages
      uses: actions/upload-artifact@v3
      with:
        name: python-package-distributions
        path: dist/